# ML Training Environment

This directory contains the ML model training infrastructure for Mentark Quantum.

## Setup

1. **Create a virtual environment:**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Set up environment variables:**
Create a `.env` file in this directory:
```env
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_service_key
MLFLOW_TRACKING_URI=file:./mlruns  # or your MLflow server URI
WANDB_API_KEY=your_wandb_key  # optional
```

4. **Run Jupyter:**
```bash
jupyter notebook
```

## Directory Structure

```
ml-training/
├── requirements.txt          # Python dependencies
├── README.md                # This file
├── notebooks/               # Jupyter notebooks for experimentation
│   ├── 01_data_exploration.ipynb
│   ├── 02_feature_engineering.ipynb
│   ├── 03_dropout_model.ipynb
│   ├── 04_burnout_model.ipynb
│   ├── 05_career_model.ipynb
│   ├── 06_difficulty_model.ipynb
│   └── 07_sentiment_model.ipynb
├── train/                   # Training scripts
│   ├── train_dropout_model.py      # XGBoost classifier
│   ├── train_burnout_model.py      # XGBoost classifier
│   ├── train_career_model.py       # LightGBM multiclass
│   ├── train_difficulty_model.py   # XGBoost regressor
│   └── train_sentiment_model.py    # TF-IDF + Logistic Regression
├── data/                    # Data files (gitignored)
│   ├── raw/
│   ├── processed/
│   └── models/
├── src/                     # Source code
│   ├── data_loader.py
│   ├── feature_engineering.py
│   ├── model_trainer.py
│   └── utils.py
└── mlruns/                  # MLflow runs (gitignored)
```

## Models to Train

1. **Dropout Risk Prediction** — XGBoost classifier (binary)
2. **Burnout Prediction** — XGBoost classifier (binary)
3. **Career Recommendation** — LightGBM classifier (multiclass)
4. **ARK Difficulty Prediction** — XGBoost regressor (continuous score)
5. **Sentiment Analysis** — TF-IDF + Logistic regression (multi/binary)

## Training Workflow

1. **Data Collection**: Features are collected via the data collection pipeline
2. **Data Labeling**: Admins label outcomes via the data labeling interface
3. **Feature Engineering**: Extract and prepare features
4. **Model Training**: Train models using training scripts
5. **Evaluation**: Evaluate models using cross-validation
6. **Deployment**: Deploy models via FastAPI endpoints

## Training the Models

Run each script from the `ml-training/train/` directory once labeled data is available:

### Dropout Risk
```bash
python train_dropout_model.py --n-estimators 200 --learning-rate 0.05
```

### Burnout Prediction
```bash
python train_burnout_model.py --n-estimators 200 --subsample 0.9
```

### Career Recommendation
```bash
python train_career_model.py --n-estimators 300 --num-leaves 63
```

### ARK Difficulty
```bash
python train_difficulty_model.py --n-estimators 400 --learning-rate 0.03
```

### Sentiment Analysis
```bash
python train_sentiment_model.py --max-features 4000
```

Each script automatically:

- Loads labeled examples from Supabase via the Python `DataLoader`
- Performs train/test splits with stratification when possible
- Computes evaluation metrics and cross-validation scores
- Saves trained models to `data/models/`
- Logs runs and artifacts to MLflow (disable with `--no-mlflow`)

## Data Requirements

- **Feature vectors**: Generated by the feature extraction pipeline (`/api/ml/feature-extraction`)
- **Labels**: Managed through the admin data-labeling dashboard (`/dashboard/admin/data-labeling`)
- **Sentiment**: Label JSON must include `text` and `label` keys (see `train_sentiment_model.py` for fallbacks)

## Shared Utilities

- `DataLoader.load_training_data(label_type)` — fetches feature vectors joined with labels
- `DataLoader.load_raw_training_records(label_type)` — raw training rows (used for sentiment)
- `utils.extract_binary_label`, `extract_multiclass_label`, `extract_regression_target` — normalize labels
- `utils.evaluate_*` helpers — standard metric reporting

## MLflow Integration

All experiments are tracked using MLflow. View runs:
```bash
mlflow ui
```

Then open http://localhost:5000 in your browser.

## Notes

- Training data is stored in Supabase `ml_training_data` table
- Feature vectors are stored in `ml_feature_store` table
- Model versions are tracked in `ml_model_versions` table
- Models are saved to `data/models/` directory


